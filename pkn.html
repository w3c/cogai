<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Plausible Knowledge Notation (PKN)</title>
  <script async class="remove" src="https://www.w3.org/Tools/respec/respec-w3c"></script>
  <script class="remove">
    var respecConfig = {
      shortName: "chunks",
      specStatus: "CG-DRAFT",
      noRecTrack: true,
      format: "markdown",
      edDraftURI: "https://w3c.github.io/cogai/",
      editors:  [
        {
          name: "Dave Raggett",
          company: "W3C",
          companyURL: "https://www.w3.org/",
          w3cid: "2682"
        }
      ],
      group: "cogai",
      github: {
        repoURL: "https://github.com/w3c/cogai/",
        branch: "master"
      },
      localBiblio: {
        "COLLINS": {
          title: "The logic of plausible reasoning: A core theory",
          year: 1989,
          publication: "Cognitive Science, 13(1), 1–49",
          href: "https://psycnet.apa.org/record/1989-38822-001",
          authors: [
            "Collins, A.",
            "Michalski, R."
          ]
        },
        "ARGUMENTATION": {
          title: "Argument and Argumentation",
          year: 2023,
          href: "https://plato.stanford.edu/entries/argument/"
        },
        "TOULMIN": {
          title: "The Uses of Argument",
          year: 1958,
          publication: "Cambridge University Press, ISBN 0-521-53483-6",
          href: "https://assets.cambridge.org/97805218/27485/sample/9780521827485ws.pdf",
          authors: [
            "Stephen Toulmin"
          ]
        },
        "WALTON": {
          title: "Argument Schemes for Presumptive Reasoning",
          year: 1996,
          publication: "Routledge, ISBN 9780805820720",
          href: "https://www.routledge.com/Argumentation-Schemes-for-Presumptive-Reasoning/Walton/p/book/9780805820720",
          authors: [
            "Douglas Walton"
          ]
        },
        "ASPIC+": {
          title: "ASPIC-END: Structured Argumentation with Explanations and Natural Deduction",
          year: 2018,
          href: "https://homepages.abdn.ac.uk/n.oren/pages/TAFA-17/papers/TAFA-17_paper_15.pdf",
          authors: [
          	"Jérémie Dauphin",
          	"Marcos Cramer"
          ]
        },
        "AIF": {
          title: "Argument Interchange Format (AIF)",
          year: 2011,
          href: "http://www.arg-tech.org/wp-content/uploads/2011/09/aif-spec.pdf"
        },
        "HAHN": {
          title: "The rationality of informal argumentation: A Bayesian approach to reasoning fallacies",
          year: 2007,
          href: "https://psycnet.apa.org/record/2007-10421-007",
          authors: [
          	"Ulrike Hahn",
          	"Mike Oaksford"
          ]
        },
        "FUZZY SETS": {
          title: "Fuzzy Sets",
          year: 1965,
          publication: "Information and Control, volume 8, 338-353 (1965)",
          href: "https://www.scribd.com/document/269570502/Zadeh-L-A-Fuzzy-Sets-1965",
          authors: [
            "Lotfi Zadeh"
          ]        },
        "IEEE-754-2019": {
          title: "IEEE 754-2019: IEEE Standard for Floating-Point Arithmetic. Institute of Electrical and Electronic Engineers, New York (2019)"
        },
        "GPT-4": {
        	title: "GPT-4",
        	year: 2023,
        	href: "https://openai.com/gpt-4"
        },
        "GENTNER": {
          title: "Similarity is like analogy: structural alignment in comparison",
          year: 1995,
          href: "https://groups.psych.northwestern.edu/gentner/papers/GentnerMarkman95b.pdf",
          authors: [
          	"Dedre Gentner",
          	"Arthur B. Markman"
          ]
        },
        "RDF Semantics": {
        	title: "RDF 1.1 Semantics",
        	year: 2014,
        	href: "http://www.w3.org/TR/rdf11-mt/",
        	authors: [
        		"Patrick J. Hayes",
        		"Peter F. Patel-Schneider"
        	]
        },
        "PKN-demo": {
          title: "Demo for plausible reasoning and argumentation",
          href: "https://www.w3.org/Data/demos/chunks/reasoning/",
          authors: [
            "Dave Raggett"
          ]
        }
      }
    };
  </script>
  <style>
table {
  border-collapse: collapse;
  border: 2px solid rgb(200,200,200);
  letter-spacing: 1px;
  font-size: 0.8rem;
}

td, th {
  border: 1px solid rgb(190,190,190);
  padding: 10px 20px;
}

th {
  background-color: rgb(235,235,235);
}

td {
  text-align: center;
}

tr:nth-child(even) td {
  background-color: rgb(250,250,250);
}

tr:nth-child(odd) td {
  background-color: rgb(245,245,245);
}
  </style>
</head>
<body>
  <section id="abstract">
    <p>This specification defines the plausible knowledge notation (PKN) as a lightweight syntax for expressing imperfect knowledge, i.e. knowledge that is uncertain, context sensitive, imprecise, incomplete, inconsistent and likely to change. PKN was inspired by the work of Alan Collins and co-workers in the 1980s, see [[COLLINS]]. PKN consists of queries, properties, relations, implications and analogies, together with named and unnamed collections of statements. PKN embraces fuzzy scalars, fuzzy modifiers and fuzzy quantifiers. PKN statements can be associated with metadata that models sub-symbolic knowledge corresponding to intuitive notations such as the strength of a statement, and the typicality of a sub-class in respect to its parent class.</p>
  </section>

  <section id="sotd">
    <p>This document is subject to change. Feedback is welcome through <a href="https://github.com/w3c/cogai/issues">GitHub issues</a> or on the <a href="mailto:public-cogai@w3.org">public-cogai@w3.org</a> mailing-list (with <a href="https://lists.w3.org/Archives/Public/public-cogai/">public archives</a>).</p>
  </section>
  
  <section class="informative">
    <h2>Introduction</h2>
    <p>This specification defines a notation and model for imperfect knowledge, such as everyday knowledge that is uncertain, context sensitive, imprecise, incomplete, inconsistent and likely to change. The approach is modelled on human argumentation, something that a long line of philosophers have investigated since the days of Ancient Greece. Traditional logic assumes perfect unchanging knowledge, and provides mathematical proof for entailments.</p>
    <p>Unfortunately, knowledge is rarely perfect, and moreover, evolves over time through learning. Nonetheless such knowledge is amenable to reasoning using guidelines for effective arguments.  As an example consider the statement: <em>if it is raining then it is cloudy</em>.  This is generally true, but you can also infer that it is somewhat likely to be raining if it is cloudy. This is plausible based upon your rough knowledge of weather patterns. In place of logical proof, we have multiple lines of argument for and against the premise in question just like in courtrooms and everyday reasoning.</p>
    
    <figure id="inferences">
      <img src="./images/inferences.png" alt="" style="width:60%"/>
      <figcaption>Inferring likely properties and relations across relations</figcaption>
    </figure>
    
    <p>The above figure shows how properties and relations involving a class may be likely to apply to a sub-class as a specialisation of the parent class. Likewise, properties and relations holding for a sub-class may be likely to apply to the parent class as a generalisation. The likelihood of such inferences is influenced by the available metadata, see Section [[[#metadata]]]. Inferences can also be based on implication rules, and analogies between concepts with matching structural relationships. PKN further supports imprecise concepts:</p>
    
    <ul>
       <li>fuzzy terms, e.g. <em>cold, warm</em> and <em>hot</em>, which form a scalar range with overlapping meanings.</li>
       <li>fuzzy modifiers, e.g. <em>very</em> old, where such terms are relative to the context they apply to.</li>
       <li>fuzzy quantifiers, e.g. <em>few</em> and <em>many</em>, for queries akin to SPARQL.</li>
    </ul>
    
    <p>For a web-based demonstrator, see [[PKN-demo]].</p>
    
    <figure id="evolution">
      <img src="./images/evolution.svg" alt="" style="width:80%"/>
      <figcaption>Evolution of Databases</figcaption>
    </figure>
    
    <p>PKN represents an evolution from graph databases to cognitive databases, that can more flexibly support reasoning over everyday knowledge, embracing uncertainty, imprecision, incompleteness and inconsistencies. Deductive proof is replaced by defeasible reasoning with arguments for and against suppositions.</p>

    <section>
      <h3>Relationship to Previous Work</h3>
      
      <p>The Stanford Encyclopedia of Philosophy entry on argument and argumentation [[ARGUMENTATION]] lists five types of arguments: deduction, induction, abduction, analogy and fallacies. Argumentation can be <em>adversarial</em> where one person tries to beat down another, or <em>cooperative</em> where people collaborate on seeking a better joint understanding by exploring arguments for and against a given supposition. The latter may further choose to focus on developing a consensus view, with the risk that argumentation may result in group polarisation when people's views become further entrenched.</p>
      
      <p>Studies of argumentation have been made by a long line of philosophers dating back to Ancient Greece, e.g. Carneades and Aristotle.  More recently, logicians such as Frege, Hilbert and Russell were primarily interested in mathematical reasoning and argumentation. Stephen Toulmin subsequently criticised the presumption that arguments  should be formulated in purely formal deductive terms [[TOULMIN]]. Douglas Walton extended tools from formal logic to cover a wider range of arguments [[WALTON]]. Ulrike Hahn, Mike Oaksford and others applied Bayesian techniques to reasoning and argumentation [[HAHN]], whilst Alan Collins applied a more intuitive approach to plausible reasoning [[COLLINS]].</p>
      
      <p>Aristotle provides guidelines for rhetorical arguments in terms of <em>ethos</em> (establishing credibility), <em>pathos</em> (appeal to emotion), <em>logos</em> (appeal to rationality), <em>kairos</em> (opportune arguments with topical references that the given audience will appreciate) together with rhetorical questions chosen to stimulate a discourse, or to draw attention and emphasise specific points. An effective argument will often involve all of these.</p> 

      <p>Formal approaches to argumentation such as [[ASPIC+]] build arguments from axioms and premises as well as strict and defeasible rules. Strict rules logically entail their conclusions, whilst defeasible rules create a presumption in favour of their conclusions, which may need to be withdrawn in the light of new information. Arguments in support of, or counter to, some supposition, build upon the facts in the knowledge graph or the conclusions of previous arguments. Preferences between arguments are derived from preferences between rules with additional considerations in respect to consistency. Counter arguments can be classified into three groups. An argument can:</p>
      
      <ul>
      <li><em><strong>undermine</em></strong> another argument when the conclusions of the former contradict premises of the latter.</li>
      <li><em><strong>undercut</em></strong> another argument by casting doubt on the link between the premises and conclusions of the latter argument.</li>
      <li><em><strong>rebut</em></strong> another argument when their respective conclusions can be shown to be contradictory.</li>
      </ul>
      
      <p>[[AIF]] is an ontology intended to serve as the basis for an interlingua between different argumentation formats. It covers <em>information</em> (such as propositions and sentences) and <em>schemes</em> (general patterns of reasoning). The latter can be used to model lines of reasoning as argument graphs that reference information as justfication. The ontology provides constraints on valid argument graphs, for example:</p>
      
	  <aside class="example">
      <dl>
      <dt><em><strong>Scheme for Argument from Expert Opinion</em></strong>:</dt> 
      <dd><em>premises</em>: E asserts that A is true (false), E is an expert in domain D containing A; <em>conclusion</em>: A is true (false); <em>presumptions</em>: E is a credible expert, A is based on evidence; <em>exceptions</em>: E is not reliable, A is not consistent with what other experts assert.</dd>
      </dl>
      </aside>

      <p>Conflict schemes model how one argument conflicts with another, e.g. if an expert is deemed unreliable, then we cannot rely on that expert's opinions. Preference schemes define preferences between one argument and another, e.g. that expert opinions are preferred over popular opinions. The AIF Core ontology is available in a number of standard ontology formats (RDF/XML, OWL/XML, Manchester OWL Syntax).</p>

      <p>PKN defines a simple notation and model for imperfect knowledge. Arguments for and against a supposition are constructed as chains of plausible inferences that are used to generate explanations. PKN draws upon Alan Collins core theory of plausible reasoning [[COLLINS]] in respect to statement metadata corresponding to intuitions and gut feelings based upon prior experience. This is in contrast to Bayesian techniques that rely on the availability of rich statistics, which are unavailable in many everyday situations.</p>
      
      <p>Recent work on large language models (LLMs), such as [[GPT-4]], have shown impressive capabilities in respect to reasoning and explanations. However, there is a risk of hallucinations, where the system presents convincing yet imaginary results. Symbolic approaches like PKN are expected to play an important and continuing role in supporting semantic interoperability between systems and knowledge graphs. LLMs are trained on very large datasets, and in principle, could be exploited to generate symbolic models in a way that complements traditional approaches to knowledge engineering.</p> 
      </section>
  </section>

<section id="conformance">
    <p>The grammatical rules in this document are to be interpreted as described in [[[RFC5234]]] [[RFC5234]].</p>
    <section>
      <h3>Conformance classes</h3>
      <p>Conformance to this specification is defined for four conformance classes:</p>
      <dl>
        <dt><dfn>PKN document</dfn></dt>
        <dd>A serialization of a [=PKN graph=] as a file. A [=PKN document=] is conformant to this specification if it follows the grammar described in <a href="#grammar"></a>.</dd>
        <dt><dfn>Authoring tool</dfn></dt>
        <dd>An application that writes a [=PKN document=]. An [=authoring tool=] is conformant to this specification if it writes conforming [=PKN documents=].</dd>
        <dt><dfn>Parser</dfn></dt>
        <dd>A [=parser=] transforms a [=PKN document=] into another representation. A [=parser=] is conformant to this specification if it can do so for any conforming [=PKN document=].</dd>
        <dt><dfn>Rule engine</dfn></dt>
        <dd>A processing application that operates on PKN statements. A [=rule engine=] is conformant to this specification if it follows the algorithms defined in Section <a href="#reasoning"></a>.</dd>
      </dl>
    </section>
</section>

<section id="data-types">
  <h2>Data Types</h2>
  
  <p>PKN documents use the following restricted set of data types. See <a href="#grammar"></a> for a formal definition of their serialization.</p>

    <p>A <dfn>number</dfn> represents a double-precision 64-bit format value as specified in the IEEE Standard for Binary Floating-Point Arithmetic [[IEEE-754-2019]]. It is serialized in base 10 using decimal digits, following the same grammar as <a data-cite="RFC8259#section-6">numbers in JSON</a> [[RFC8259]].</p>
    
    <p>A <dfn>name</dfn> is a string that can include letters, digits, colon, period, hyphen, underscore and slash characters, and that cannot be interpreted as a [=number=].</p>
    
    <p class="issue">Should this be expanded to cover additional data types such as ISO8601 dates and string literals?  That might be convenient in some respects, but it would at the same time introduce complications in respect to how to operate on such data types. It might be simpler to handle such data types at the periphery of the system, e.g. mapping dates to a set of properties, and likewise handling string literals via a lexical dictionary.</p>
</section>

<section id="statements">
  <h2>PKN Statements</h2>
  
  <p>PKN supports several kinds of statements as defined below along with named and unnamed collections of statements.</p>

  <section id="metadata">
    <h3>Statement MetaData</h3>
    
    <p>Property, relation and implication statements optionally have a scope and a set of parameters. The scope is one or more names that indicate the context in which the statement applies, e.g. ducks are similar to geese in that they are birds with relatively long necks when compared to other bird species. Each parameter consists of a name and a value. Parameters represent prior knowledge as an informal qualitative feeling. Predefined parameters include:</p>
    
    <dl>
    <dt><dfn>certainty</dfn></dt>
    <dd>The confidence in the associated statement being true.</dd>
    <dt><dfn>strength</dfn></dt>
    <dd>The confidence in the consequents being true for an implication statement, i.e. the likelihood of the consequents holding if the antecedents hold.</dd>
    <dt><dfn>inverse</dfn></dt>
    <dd>The confidence in the antecedents being true when using an implication statement in reverse, i.e. the likelihood of the antecedents holding if the consequents hold.</dd>
    <dt><dfn>typicality</dfn></dt>
    <dd>The likelihood that a given instance of a class is typical for that class, e.g. that a Robin is a typical song bird.</dd>
    <dt><dfn>similarity</dfn></dt>
    <dd>The extent to which one thing is similar to another, e.g. the extent that they have some of the same properties.</dd>
    <dt><dfn>dominance</dfn></dt>
    <dd>The relative importance of an instance of a class as compared to other instances. For a country, for instance, this could relate to the size of its population or the size of its economy.</dd>
    <dt><dfn>multiplicity</dfn></dt>
    <dd>The number of items in a given range, e.g. how many different kinds of flowers grow in England, remembering that parameters are qualitative not quantitative.</dd>
    </dl>
    
    <p>See section [[[#reasoning]]] for the role of statement parameters in argumentation.</p>
    
     <p class="issue">Need to explain the relation to existing argumentation systems, along with formalisms such as AIF and ASPIC, together with questions around correctness, complexity and semantics. Note the need to explain why formal semantics is challenging for imperfect knowledge.</p>
  </section>

  <section id="properties">
    <h3>Property Statements</h3>
    
    <p>These are used either to declare properties of things, or as conditions and actions in implication and query statements. A property statement has four parts: the descriptor, an argument, an operator and a referent. Here is an example:</p> 
    
    <aside class="example" title="Property Statements">
      <pre class="pkn">flowers of Netherlands includes daffodils, tulips (certainty high)</pre>
      <p>Here <em>flowers</em> is the descriptor, <em>Netherlands</em> is the argument, <em>includes</em> is the operator, and <em>daffodils, tulips</em> is the referent. In other words, daffodils and tulips are amongst the flowers found in the Netherlands. The meta data indicates that this statement has a high certainty.</p>
    </aside>
    
    <p>The following operators are predefined:</p>
    <dl>
    <dt><dfn>includes</dfn></dt>
    <dd>This implies that the property is not limited to the given referent.</dd>
    <dt><dfn>excludes</dfn></dt>
    <dd>This implies that the property doesn't hold for the given referent.</dd>
    <dt><dfn>is</dfn></dt>
    <dd>This implies that the property is limited to the given referent.</dd>
    </dl>
    
    <p>Note: variables are only permitted within properties that appear as conditions or actions in implication statements or query statements.</p>
    
    <p class="issue">Should the the predefined operators include "<code>is not</code>"?</p>
  </section>

  <section id="relations">
    <h3>Relation Statements</h3>
    
    <p>Relation statements have three parts: the subject, the relationship, and the object. Relations can be used on their own, or as part of implication and query statements. Here are some examples:</p>
    
    <aside class="example" title="Relation Statements">
      <pre class="pkn">tulips kind-of temperate-flowers
Belgium similar-to Netherlands for latitude
younger-than equivalent-to less-than for age</pre>
      <p>The first example declares that tulips are a kind of flowers found in countries with temperate climates. The last two examples specify a scope for which the relation holds.</p>
    </aside>
    
    <p>Use metadata to indicate the strength of a relation, see [=strength=] and [=inverse=].  Use prefixes on the relationship as modifiers, i.e. similar to adjectives when applied to a noun, as in:</p>
    
    <aside class="example">
      <pre class="pkn">
Paul close:friend-of John</pre></p>
    </aside>
    
    <p class="issue">Is there a need to a separate means to model negation or are prefixes sufficient?</p>
    
    <p>Note: variables are only permitted within relations that appear as conditions or actions in implication statements or query statements.</p>
  </section>

  <section id="implications">
    <h3>Implication Statements</h3>
    
    <p>An implication statement defines one or more consequents, and one or more antecedents, which are properties or relations, and may include variables which are scoped to the implication statement. The consequents are evaluated as a conjunction of conditions. See section [[[#reasoning]]] for details. Here are some examples of implication statements:</p>
    
    <aside class="example" title="Implications Statements">
      <pre class="pkn"># rainy weather and cloudy conditions
weather of ?place includes rainy
	implies weather of ?place includes cloudy (strength high, inverse low)</pre>
      <p>This example has a single antecedent and a single consequent. Note the use of <code>?place</code> as a variable, and metadata for the confidence in using the statement for forward and backward inferences.</p>
    </aside>
    
    <p class="issue">Whilst disjunctions can be handled via separate rules or relations, would it make sense to directly support disjunctions in rule antecedents? How would that generalise further, taking imprecision and fuzzyness into account?</p>
  </section>

  <section id="analogies">
    <h3>Analogy Statements</h3>
    <p> These relate two pairs of concepts where each pair stands as an analogy for the other pair.</p>
    
    <aside class="example" title="Analogy Statements">
      <pre class="pkn">leaf:tree::petal:flower</pre>
      <p>In this example, a leaf is a part of a tree, whilst a petal is part of a flower. The analogy holds because both pairs of concepts involve the <em>part-of</em> relation.</p>
    <p>Note that concepts can be replaced by variables when you want to query for a concept that fits the implicit analogy, as shown below.</p>
    <pre class="pkn">dog:puppy::cat:?</pre>
    </aside>
  </section>
  
  <section id="queries">
    <h3>Query Statements</h3>
    <p>These have a <em>quantifier</em>, a <em>variable</em>, a required <em>where clause</em>, and an optional <em>from clause</em>. Variables are scoped to the query statement. The where and from clauses involve a conjunction of one or more conditions, which are properties or relations. The conditions for the <em>from</em> clause are evaluated against the PKN statement graph. The conditions for the <em>where</em> clause are evaluated against the matches found with the <em>from</em> clause if present, otherwise against the PKN statement graph. The tests relate to the variable bindings.</p>
    
    <aside class="example" title="Query Statements">
      <pre class="pkn">which ?x where ?x is-a person and age of ?x is very:old
count ?x where age of ?x greater-than 20 from ?x is-a person
few ?x where color of ?x includes yellow from ?x kind-of rose</pre>
      <p>The first query lists the people in the PKN graph who are considered to be very old. See section [[[#fuzzy]]]. The second query counts the number of people older that 20. The third query checks whether there are few yellow roses in the PKN graph.</p>
    </aside>
    
    <p>Queries can use the following quantifiers:</p>
    
    <dl>
    <dt><dfn>no</dfn></dt>
    <dd>Tests that the PKN graph has no matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>all</dfn></dt>
    <dd>Tests that the conditions for the  <em>where clause</em> hold for all of the matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>any</dfn></dt>
    <dd>Tests that the conditions for the  <em>where clause</em> hold for at least one of the matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>few</dfn></dt>
    <dd>Tests that the conditions for the  <em>where clause</em> hold for few of the matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>many</dfn></dt>
    <dd>Tests that the conditions for the  <em>where clause</em> hold for many of the matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>most</dfn></dt>
    <dd>Tests that the conditions for the  <em>where clause</em> hold for most of the matches for the conditions in the <em>from clause</em>.</dd>
    <dt><dfn>which</dfn></dt>
    <dd>Yields the list of variable bindings that satisfy the conditions.</dd>
    <dt><dfn>count</dfn></dt>
    <dd>Yields the length of the list of variable bindings that satisfy the conditions.</dd>
    </dl>
    
    <p>The meaning of the quantifiers is for the most part heuristically determined by the relative size of the sets of variable bindings for the <em>where</em> and <em>from</em> clauses respectively. The exceptions are <em>no, which</em> and <em>count</em>. Note that <em>few, many</em> and <em>most</em> are intentionally imprecise, see Section [[[#fuzzy-quantifiers]]].</p>
    
    <p class="issue">What are the implications of allowing a comma separated list of quantifier variables?</p>
  </section>
  
  <section id="named-statements">
    <h3>Named Collections of Statements</h3>
    <p>This allow properties and relations to refer to named collections of statements, e.g. when modelling reported speech.</p>  
    
    <aside class="example" title="Statement Collections">
      <pre class="pkn">
g1: {
	Fido is-a dog
	Mia is-a cat
}

Michael said g1</pre>
      <p>This is an example of a named collection of statements where the name is "g1" and the statements assert that Fido is a dog and Mia is a cat.</p>
    </aside>
    <p>Unnamed collections of statements can be used in situ as part of properties and relations, e.g.</p>
    <aside class="example" title="Unnamed Statement Collections">
      <pre class="pkn">Peter believes {Wendy likes chocolate}</pre>
    </aside>
  </section>

</section>

<section id="fuzzy" class="informative">
	<h2>Fuzzy Knowledge</h2>
	
	<p>Plausible reasoning subsumes fuzzy logic as expounded by Lotfi Zadeh in his 1965 paper on fuzzy logic, see [[FUZZY SETS]]. Fuzzy logic includes four parts: fuzzification, fuzzy rules, fuzzy inference and defuzzification.</p>
	<p>Fuzzification maps a numerical value, e.g. a temperature reading, into a fuzzy set, where a given temperature could be modelled as 0% cold, 20% warm and 80% hot. This involves transfer functions for each term, and may use a linear ramp or some kind of smooth function for the upper and lower part of the term’s range.</p>
	<p>Fuzzy rules relate terms from different ranges, e.g., if it is hot, set the fan speed to fast, if it is warm, set the fan speed to slow. The rules can be applied to determine the desired fan speed as a fuzzy set, e.g., 0% stop, 20% slow and 80% fast. Defuzzification maps this back to a numeric value.</p>
	<p>Fuzzy logic works with fuzzy sets in a way that mimics Boolean logic in respect to the values associated with the terms in the fuzzy sets. Logical AND is mapped to selecting the minimum value, logical OR is mapped to selecting the maximum value, and logical NOT to one minus the value, assuming values are between zero and one.</p>
	<p>Plausible reasoning expands on fuzzy logic to support a much broader range of inferences, including context dependent concepts, and the means to express fuzzy modifiers and fuzzy quantifiers.</p>
	
	<section>
	    <h2>Scalar ranges</h2>
	    <p>Some concepts are characterised by a value that lies in a range. Examples include temperature, age and climate. The temperature of a room may be described as cold, warm or hot. Such terms are imprecise and lack a formally agreed definition. Moreover, the meaning may depend on the context. Someone who is 60 years old will have a very different notion of what "old" means compared to that of an eight year old child.</p>
	    
	    <p>The choice of scales for a range may depend on the application. Here are some examples of scales for different types of climate:</p>
	    
	    <ul>
	    <li>dry, temperate, continental and polar</li>
	    <li>polar, temperate, arid, tropical, mediterranean, mountain</li>
	    <li><em>Hot:</em> equatorial, tropical, subtropical; <em>Temperate:</em>  mediterranean. chinese, oceanic, continental; <em>Cold:</em> polar, highland</li>
	    </ul>
	    
	    <p>Each of these terms is associated with typical weather patterns for that climate, e.g. in cities like Shanghai, Buenos Aires, Sydney, and Hong Kong there is a so called Chinese climate with mild winters and humid summers with tropical rain.</p>
	    
	    <p>This can be modelled using the <em>range</em> property along with a <em>scope</em>. The value of this property is a list of climates. These climates can be treated as arguments for properties that characterise that climate, e.g. in terms of their span on a numerical scale.</p>
	    
    
        <p>Note that descriptions may involve terms describing different aspects, e.g. temperature and humidity, that belong to separate ranges, e.g. cold and wet in contrast to hot and dry.</p>

	    <p>Here is an example for <em>age</em> that lists terms for describing someone's age, along with the associate span in years:</p>
	    
    <aside class="example" title="Scalar Ranges">
      <pre class="pkn"><code>range of age is infant, child, adult for person
age of infant is 0, 4 for person
age of child is 5, 17 for person
age of adult is 18, age-at-death for person</code></pre>
      <p>The <em>range</em> property lists the terms used for different categories. The <em>age</em> property for the terms then specifies the numerical range. Additional properties can be used to define the transfer function.</p>
    </aside>
    </section>
    
    <section>
	    <h2>Fuzzy Modifiers</h2>
	    <p>PKN allows terms to be combined with one or more fuzzy modifiers, e.g. <code>very:old</code> where <em>very</em> acts like an adjective when applied to a noun. The meaning of modifiers can be expressed using PKN statements for relations and implications, together with scopes for context sensitivity. In respect to <em>old</em>, "very" could either be defined by reference to a term such as "geriatric" as part of a range for "age", or with respect to the numerical value, e.g. greater than 75 years old.</p>
	</section>
	<section>
	    <h2>Fuzzy Quantifiers</h2>
	    
	    <p>These are quantifiers, such as <em>few, many</em> and <em>most</em>, with an imprecise meaning. See Section [[[#queries]]]. Their meaning can be defined in terms of the length of the list of query variable bindings that satisfy the conditions. <code>few</code> signifies a small number, <code>many</code> signifies a large number, and <code>most</code> signifies that the number of bindings for the <em>where clause</em> is a large proportion of the number of bindings for the <em>from clause</em>.</p>
	    
	<p class="ednote">Add some examples for how to express the meaning of fuzzy modifiers and fuzzy quantifiers.</p>
    
    <p class="issue">Is it necessary to predefine the operational meaning of fuzzy quantifiers and fuzzy modifiers? It may be preferable to require applications to define the meaning according to their needs as the meaning is likely to be context sensitive.</p>
	</section>
	

</section>

<section id="documents">
  <h2>PKN Documents</h2>
  
  <p>A PKN document is a sequence of PKN statements and named collections of statements.</p>

  <section>
    <h3>PKN Graphs</h3>
    <p>A <dfn data-lt="pkn-graph">PKN Graph</dfn> is an abstract representation of a PKN document as a graph in which statement are the vertices and names are the edges.</p>
  </section>
  
  <section id="grammar">
    <h3>PKN Grammar</h3>
    <p>A [=PKN document=] MUST follow the grammar defined below. White space MAY be included between terms, and MUST be used when needed to separate adjacent terms, e.g. a name followed by a value. White space is NOT permitted within names or numbers, e.g. between a prefix and a suffix. White space is likewise NOT permitted between '?' and the name token for a variable, nor between the sign ('+' and '-') and the digits that form a number. Comments start with '#' and continue to the end of the current line or the end of the file, whichever comes first.</p>

    <pre class="pkn"><code class="abnf" data-include="grammar/pkn.ebnf" data-include-format="text"></code></pre>
    
    <p class="issue">Should the grammar explicitly define where white space is optional or required?</p>
  
    <p class="issue">Should the grammar notation be switched from EBNF to ABNF for consistency with W3C practice, noting that EBNF is easier to understand?</p>
  </section>  
  
  <section id="railroad-diagrams" class="informative">
    <h3>Railroad Diagrams</h3>
    
    <p>This section presents a visualisation of the grammar in the form of railroad diagrams. These diagrams are provided solely to make it easier to get an intuitive grasp of the syntax of each token.</p>
    
    <div data-include="grammar/pkn-rr.html" data-include-replace="true"></div>
  </section>  
  
  <section id="pkn-rdf" class="informative">
    <h3>PKN and RDF</h3>
    
    <p>The Resource Description Framework (RDF), see [[rdf-concepts]], defines a data model for labelled directed graphs, along with serialisation formats such as Turtle, query expressions with SPARQL, and schemas with RDF-S, OWL and SHACL. RDF identifiers are either globally scoped (IRIs) or locally scoped (blank nodes). RDF literals include numbers, booleans, dates and strings. String literals can be tagged with a language code or a data type IRI.</p>
    <p>The semantics of RDF graphs is based upon Description Logics, see [[RDF Semantics]] and [[owl2-rdf-based-semantics]]. RDF assumes that everything that is not known to be true should treated as unknown. This can be contrasted with closed contexts where the absence of some statement implies that it is not true.</p>
    <p>Description Logics are based upon deductive proof, whereas, PKN is based upon defeasible reasoning which involves presumptions in favour of plausible inferences, and estimating the degree to which the conclusions hold true. As such, when PKN graphs are translated into RDF, defeasible semantics are implicit and dependent on how the resulting graphs are interpreted. Existing tools such as SPARQL don't support defeasible reasoning.</p>
    <p>Consider PKN property statements. The descriptor, argument operator and referent, along with any statement metadata can be mapped to a set of RDF triples where the subject of the triples is a generated blank node corresponding to the property statement. Comma separated lists for referents and scopes can be mapped to RDF collections.</p>
    <p>PKN relations statements can be handled in a similar manner. It might be tempting to translate the relation's subject, relationship and object into a single RDF triple, but this won't work when the PKN relation is constrained to a scope, or is associated with statement metadata.</p>
    <p>PKN implication statements are more complex to handle as they involve a sequence of antecedents and a sequence of consequents, as well as locally scoped variables. One possible approach is to first generate a blank node for the statement, and use it as the subject for RDF collections for the variables, antecedents and consequents.</p>
    <p>PKN analogy statements are simpler, although there is a need to be able to distinguish variables from named concepts, e.g. as in <code>"dog:puppy::cat:?"</code>.</p> 
    <p class="ednote">Need to explain how PKN relates to RDF-star and N3.</p>
  </section>  
</section>

<section id="reasoning" class="informative">
  <h2>Plausible Reasoning and Argumentation</h2>
  
  <p>Plausible reasoning is about generating arguments that support or counter a premise, gathering information along the way. The reasoning process is driven by a queue of premises awaiting consideration, plus a record of reasoning so far, which can be modelled as a reasoning graph. The reasoner applies heuristics (strategies and tactics) to determine which potential inferences to apply and in what order to develop the arguments. As reasoning proceeds, the consideration of additional evidence may either strengthen or weaken earlier conclusions.</p>
    
  <section id="explanations">
    <h3>Generating Explanations</h3>
    <p>The inference engine can start with the premise in question and  look for direct evidence, and after that, indirect evidence involving inferences using different kinds of PKN statements that pose intermediate premises for consideration. The reasoning process generates a graph that starts from the premises and works back to direct evidence in the knowledge graph. The reasoning graph can then be scanned in the reverse direction to generate explanations that start with the facts and progressively justify the premise via a series of inferences.</p>
    <p>The reasoning process can try to support an argument via finding additional evidence, or it can try to generate counter arguments that undermine, undercut or rebut other arguments. The explanation then needs to put these arguments into context using the appropriate templates.</p>
  </section>  
  
  <section id="certainty">
    <h3>Estimating Certainty</h3>
  <p>The next few sections describe how PKN statements can be used to compute inferences along with their estimated certainty, as the basis for constructing arguments both for and against a given premise, in terms of chains of inferences.</p>
  <p>In probability theory the conditional probability of event A occurring given an occurrence of event B can be expressed using Bayes theorem:</p>
  
    <figure id="bayes">
      <img src="./images/bayes.svg" alt="" style="width:30%"/>
      <figcaption>Bayes Theorem</figcaption>
    </figure>
      
  <p>When applied to inferences, this requires the gathering of lots of statistics which is very challenging from a practical perspective. In the absence of comprehensive statistics, the certainty to which the conclusions hold true can be informally modelled as a number in the range 0 to 1. Qualitative terms such as low, medium and high can be mapped to such numbers for use in estimation algorithms, and later when needed, mapped back into qualitative terms.</p>
  <p>One such algorithm is where a premise directly matches multiple properties with non-exclusive values, which can then be combined as a set. The more matches, the greater the certainty for establishing the goal.</p>
  <p>If c is the average certainty and n is the number of matches, the
combined certainty is 1.0 - (1.0 - c)/n. If c = 0.5 and n = 1 we get 0.5. If n = 2, we get 0.75. If n = 4, we get 0.85, If n = 256, we get 0.998.</p>
  <p>Collins et al. suggest a simplified approach to modelling the effects of the various metadata parameters in which each parameter strengthens, weakens or has no effect on the estimated certainty, and using the same algorithm for all such parameters.</p>
  <p>If the original certainty is zero, the parameters should have no effect, and likewise, the effect of a parameter should be smaller as the parameter’s value tends to zero.</p>
  <p>Treating the effect of a parameter as a multiplier m on the certainty, the number should be in the range 0 to 1/c, where c > 0. If we want to boost c by 25% when c = 0.5, m = 1.25, but this should shrink to 1 when c = 1.</p>
  <p>How much should m increase as c tends to 0? One idea is to use linear interpolation, i.e., 1.5 when c = 0, 1.25 when c = 0.5 and 1 when c = 1. This multiplier shrinks to 1 as the value of the parameter tends to 0 and when c tends to 0. Thus m = 1 + p/2 - p*c/2, where m is the multiplier, p is the parameter value (0 to 1) and c is the certainty (0 to 1).</p>
  <p>We also need to deal with multiple lines of argument for and against the premise in question. If the arguments agree, we can aggregate their certainties using the first algorithm above. We are then left with a fuzzy set for the different conclusions, e.g. (true 0.8, false 0.2). Note that arguments may present multiple conclusions rather than true or false, as depending on the query posed to the reasoner.</p>
  </section>  
  
  <section id="reasoning-proprels">
    <h3>Reasoning with Properties and Relations</h3>
    
    <p>The reasoner can be given a property or relationship as a premise that we want to find evidence for and against, for instance, here is a premise expressed as a property:</p>
    
      <aside class="example">
      <pre class="pkn">
flowers of England includes daffodils   
      </pre>
      </aside>
      
     <p>Its inverse would be:</p>
    
      <aside class="example">
      <pre class="pkn">
flowers of England excludes daffodils
      </pre>
      </aside>
      
     <p>The reasoner first checks if the premise is a known fact, and if not looks for other ways to gather evidence. One tactic is to generalize the property referent, e.g., by replacing it with a variable as in the following:</p>
    
      <aside class="example">
      <pre class="pkn">
flowers of England includes ?flower
      </pre>
      </aside>

     <p>The knowledge graph provides a matching property statement:</p>
     
      <aside class="example">
      <pre class="pkn">
flowers of England includes temperate-flowers
      </pre>
      </aside>

     <p>We then look for ways to relate daffodils to temperate flowers, finding a matching relation statement:</p>
     
      <aside class="example">
      <pre class="pkn">
daffodils kind-of temperate-flowers
      </pre>
      </aside>

     <p>So, we have inferred that daffodils grow in England. Another tactic is to generalize the property argument as in the following:</p>
     
      <aside class="example">
      <pre class="pkn">
flowers of ?place includes daffodils
      </pre>
      </aside>

     <p>We can then look for ways to relate England to similar countries, for example:</p>
     
      <aside class="example">
      <pre class="pkn">
Netherlands similar-to England for flowers
      </pre>
      </aside>

     <p>We find then a match, for example:</p>
     
      <aside class="example">
      <pre class="pkn">
flowers of Netherlands includes daffodils, tulips, roses
      </pre>
      </aside>

     <p>Thus, providing us with a second way to infer that daffodils grow in England. The certainty depends on the parameters, and the [=similarity=] parameter in respect to the similar-to relation. A further tactic is to generalise the descriptor, i.e. replacing <em>flowers</em> by <em>?flowers</em>, and then look for evidence that relates to flowers.</p>
     
     <p>If the premise is a property whose referent is a variable, then its binding may combine values from lines of evidence, e.g. roses from one line of reasoning, and daffodils and tulips from another. This depends on the property operator, e.g. [=includes=] supports such combination, whereas [=excludes=] precludes combination.</p>
     
     <p>Whilst the above uses a property as a premise, you can also use relations as premises, for instance:</p>
     
      <aside class="example">
      <pre class="pkn">
Peter is young
      </pre>
      </aside>

     <p>The reasoner will then look for relevant knowledge on whether Peter is, or is not, young, e.g., Peter’s age, and whether that implies he is a child or an adult, based upon pertinent range statements and associated definitions for their values.</p>
     
     <p>In a small knowledge graph, it is practical to exhaustively consider all potentially relevant inferences. This becomes increasingly costly as the size of the knowledge graph increases. One way to address this challenge is to prioritize inferences that seem more certain, and to ignore those that are deemed too weak.</p>
  
    <p>See [[[#inferences]]] for an illustration of how properties and relations can be plausibly inferred across relation statements such as <em>kind-of</em> depending on the associated metadata. An instance or subclass of a class can plausibly share properties of that class as a specialisation, especially if that instance or subclass is annotated as being typical of that class, see [=typicality=]. Likewise, a class can plausibly share properties of a subclass as a generalisation.</p>
    
    <p>If two concept are similar, then they are likely to share the same properties according to their degree of [=similarity=]. You can restrict this to a named context, e.g. as in:</p>
    
      <aside class="example">
      <pre class="pkn">
duck similar-to goose for neck-length
duck dissimilar-to goose for sound      
      </pre>
      </aside>

    <p>Here are some examples of PKN relationships with their meaning, and whether the meaning is symmetric or asymmetric.</p>
    
    <table border>
    <tr>
    <th>Relationship Type</th>
    <th>Meaning</th>
    <th>Reversible</th>
    </tr>
    <tr>
    <td>kind-of</td>
    <td>sub class</td>
    <td>asymmetric</td>
    </tr>
    <tr>
    <td>is-a</td>
    <td>instance of class</td>
    <td>asymmetric</td>
    </tr>
    <tr>
    <td>similar-to</td>
    <td>similar concepts</td>
    <td>symmetric</td>
    </tr>
    <tr>
    <td>dissimilar-to</td>
    <td>dissimilar concepts</td>
    <td>symmetric</td>
    </tr>
    <tr>
    <td>depends-on</td>
    <td>functional dependency</td>
    <td>asymmetric</td>
    </tr>
    <tr>
    <td>analogous-to</td>
    <td>analogous concepts</td>
    <td>symmetric</td>
    </tr>
    <tr>
    <td>opposite-to</td>
    <td>opposite meaning</td>
    <td>symmetric</td>
    </tr>
    </table> 
    
    <p class="issue">The reasoner can exploit built-in tactics for reasoning, but it would also be interesting to provide a declarative means to describe such tactics, for instance, as an enabler for machine learning.</p>
  </section>  
  
  <section id="reasoning-implications">
    <h3>Reasoning with Implications</h3>
    
    <p>Implication rules are conditioned on conjunctions of antecedents, which can be properties or relations, see the folowing example.</p>
    
      <aside class="example">
      <pre class="pkn">
?adult is-a adult and age of ?adult less-than 25 implies age of ?adult is very:young
      </pre>
      </aside>

    <p>Disjunctions can be modelled using multiple implications. Negatives can expressed using antonyms (terms with an opposite meaning to a given term). The [=strength=] parameter describes the conditional likelihood of the consequents given the antecedents, whilst [=inverse=] describes the conditional likelihood of the antecedents given the consequents.</p>
      
    <p>An implication can be chosen for consideration when one of its consequents matches a premise in question, the reasoner then needs to seek evidence to establish all of the antecedents before binding any variables in that premise and updating its estimated certainty. A similar process applies when using the implication in reverse.</p>
  </section>  
  
  <section id="reasoning-analogies">
    <h3>Reasoning with Analogies</h3>
    <p>Analogies involve comparisons between things or objects where similarities in some respects can suggest the likelihood of similarities in other respects, for instance, in their respective properties and relationships, or in ways to solve related problems. Gentner and Markman proposed a basis for modelling analogies in terms of establishing a structural alignment based on common relational structure for two representations, where the stronger the match, the better the analogy, see [[[GENTNER]]].</p>
    <p>In some cases, the objects may have the same properties, in other cases, there is a systematic mapping between different properties, e.g., relating electric current to the flow of a liquid in a pipe, and correspondingly, voltage to pressure. If you know that flow increases with pressure, you can infer that current increases with voltage.</p>
    <p>Analogical reasoning tests often take the form of A is to B as X is to —, where students are asked to supply the missing term. This can be readily modelled using PKN, for example consider the query:</p>
    
      <aside class="example">
      <pre class="pkn">
# leaf is to tree as petal is to what?

leaf:tree::petal:?
      </pre>
      </aside>
    
    <p>when applied to the knowledge:</p>
    
      <aside class="example">
      <pre class="pkn">
leaf part-of tree
petal part-of flower
      </pre>
      </aside>
    
    <p>The reasoner finds that <em>leaf</em> is related to <em>tree</em> via the <em>part-of</em> relationship, and can then use that to look for a part-of relationship involving petal, yielding the result flower. Now consider:</p>
    
      <aside class="example">
      <pre class="pkn">
like:love::dislike:?
      </pre>
      </aside>
    
    <p>when applied to the knowledge:</p>
    
      <aside class="example">
      <pre class="pkn">
love more-than like
hate more-than dislike
      </pre>
      </aside>
    
    <p>This gives the answer hate by matching the object of the relationship rather than the subject. Here is a more complex example showing the output from the reasoner:</p>
    
      <aside class="example">
      <pre class="pkn">
Premise: mansion:shack::yacht:?

Found: mansion is large for building
Found: shack is small for building

So contrast is large, small

Found: yacht is large for sailing-boat
Found: dingy is small for sailing-boat

Therefore mansion:shack::yacht:dingy
      </pre>
      </aside>
      
    <p>PKN also allows you to reason from known analogies, e.g. given:</p>
    
      <aside class="example">
      <pre class="pkn">
circuit analogous-to plumbing
flow increases-with pressure for plumbing
current increases-with voltage for circuit
      </pre>
      </aside>
    
    <p>The reasoner can then infer the following:</p>

      <aside class="example">
      <pre class="pkn">
voltage corresponds-to pressure
current corresponds-to flow
      </pre>
      </aside>

    <p>Analogies involve a form of inductive reasoning, and is related to learning from examples, and being able to apply past experience to a new situation based upon noticing the similarities and reasoning about the differences. Such reasoning can be simple as in the above examples or more complicated, for instance, when involving causal modelling.</p>
  </section>  
  
  <section id="reasoning-queries">
    <h3>Reasoning with Queries</h3>
    
    <p>Query statements are loosely analogous to SPARQL for RDF graphs. The reasoner seeks evidence to bind the quantified variables and applies the quantifier to generate the query results. Consider the example:</p>
    
       <aside class="example">
      <pre class="pkn">
few ?x where color of ?x includes yellow from ?x kind-of rose
      </pre>
      </aside>

    <p>when applied to the following facts:</p>
    
       <aside class="example">
      <pre class="pkn">
rose-blue-moon kind-of rose
color of rose-blue-moon includes purple
rose-pascali kind-of rose
color of rose-pascali includes white
rose-lovers-meeting kind-of rose
color of rose-lovers-meeting includes orange
rose-ruby-wedding kind-of rose
color of rose-ruby-wedding includes red
rose-wendy-cussons kind-of rose
color of rose-wendy-cussons includes magenta
rose-fragrant-cloud kind-of rose
color of rose-fragrant-cloud includes red
rose-peace kind-of rose
color of rose-peace includes yellow, pink
rose-double-delight kind-of rose
color of rose-double-delight includes white, red
rose-sublest kind-of rose
color of rose-sublest includes yellow
rose-harry-wheatcroft kind-of rose
color of rose-harry-wheatcroft includes red, yellow
      </pre>
      </aside>
    
    <p>The variable <code>?x</code> binds to 3 kinds of yellow roses out of a total of 10 known varieties of roses, so for this dataset at least, the query evaluates to true as 3 is small compared to 10. For more details, see Section [[[#queries]]].</p>
    
    <p>Further work is needed to support queries involving superlatives, e.g. what is the <em>highest</em> mountain in the world?. The associated scalars (in this case "height") may be associated with antonyms, e.g. low/high, short/tall, small/large. A counter example is "sweetness", i.e. how sugary some drink or food is perceived to be.  We can ask which is the sweetest fruit, but there is no word that describes its opposite. In principle, such queries could be enabled through statements that declare the superlative for a given term, e.g. <em>shortest is-superlative-for short</em>.</p>
  </section>  

  <section id="reasoning-graphs">
    <h3>Reasoning about Graphs</h3>
    
    <p>Consider the statement: "Mary thinks John loves Joan". We can model this using a graph for Mary's beliefs as the object of a relation:</p>
    
       <aside class="example">
      <pre class="pkn">
Mary believes { John loves Joan }
      </pre>
      </aside>
    
    <p>Here is a more complicated example "Mary thinks John is lying when he says he loves Joan"</p>
    
       <aside class="example">
      <pre class="pkn">
Mary believes {{John says {John loves Joan}} is-a lie}
      </pre>
      </aside>
      
    <p>Sometimes it will be more convenient to define a named collection of statements, and use the name to refer to them from PKN statements.</p>
      
       <aside class="example">
      <pre class="pkn">
g2: {
	John likes coffee
	Sue likes tea
}

Joan said g2</pre>
      </aside>
      
    <p>Here is an example of a query involving a graph:</p>

       <aside class="example">
      <pre class="pkn">
which ?x where Joan said {?x likes tea}
      </pre>
      </aside>
    <p>where the answer “Sue” is found by searching through the graph named “g2”.</p>
    <p>A more complex case is where reasoning is in the context of a fictional world in which some things are specific to that world whilst most things are the same as in our world. An example is the world of the Harry Potter novels where magic is real, but only some people can use it.  This then involves reasoning across multiple graphs according to their relationships. This is also the case for reasoning about the future, e.g. when developing plans for how to achieve a goal, and for abductive reasoning about the likely causes of some  observations, where we need to deal with what might be true given some plausible assumptions.</p>
  </section>  

  <section id="arguments">
    <h3>Assessing Multiple Lines of Argumentation</h3>
    <p>PKN reasoners can generate arguments for and against a given premise, where each argument is formed from a chain of plausible inferences. The reasoning process estimates the certainty of each argument. Arguments can be grouped for the purposes of generating explanations from the argument graph. If the certainties for and against the premise are roughly similar, then the users are left to draw their own conclusions. The reasoner should provide further details when asked.</p>
  </section> 

  <section id="where-next">
    <h3>Where Next?</h3>
    
    <p>An open challenge is how to declaratively model strategies and tactics for reasoning rather than needing to hard code them as part of the reasoner's implementation. Further work is needed to clarify the requirements and to evaluate different ways to fulfil those requirements using an intuitively understandable syntax. The [[AIF]] ontology would be a useful source of inspiration.</p>
    
    <p>This raises the question of how enable machine learning for both knowledge graphs and reasoning strategies and tactics.</p>
    
    <p>Advances in large language models have shown impressive reasoning capabilities and can readily handle complex higher order statements, such as above. This suggests that large scale machine learning is much more tractable using vector space representations instead of symbolic graphs. On the other hand, symbolic knowledge has a continuing role in relation to transparency and semantic interoperability. How can the two approaches be combined to support collaborative knowledge engineering using neurosymbolic information systems?</p>
  </section>  
</section>

</body>
</html>
